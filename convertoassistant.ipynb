{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import ast\n",
    "import traceback  # Import traceback for detailed error information\n",
    "import json\n",
    "import weaviate\n",
    "import re\n",
    "from transformers import GPT2Tokenizer\n",
    "import requests\n",
    "from werkzeug.utils import secure_filename\n",
    "import os\n",
    "import fitz\n",
    "import uuid\n",
    "import textwrap\n",
    "import builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openai = openai.OpenAI(api_key=\"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\") # be sure to set your OPENAI_API_KEY environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wprint(*args, width=70, **kwargs):\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "    wrapped_args = [wrapper.fill(str(arg)) for arg in args]\n",
    "    builtins.print(*wrapped_args, **kwargs)\n",
    "\n",
    "# Function to execute a thread and retrieve the completion\n",
    "def get_completion(message, agent, funcs, thread, client):\n",
    "    # Create new message in the thread\n",
    "    message_response = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=message\n",
    "    )\n",
    "\n",
    "    # Run the thread\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=agent.id,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        # Wait until run completes\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id\n",
    "        )\n",
    "\n",
    "        if run.status in ['queued', 'in_progress']:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        if run.status == \"requires_action\":\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                print(f\"Debug: Calling function {tool_call.function.name}\", flush=True)\n",
    "\n",
    "                wprint(f'\\033[31mFunction: {tool_call.function.name}\\033[0m')\n",
    "                func = next((f for f in funcs if f.__name__ == tool_call.function.name), None)\n",
    "                if func:\n",
    "                    try:\n",
    "                        # Assuming arguments are parsed correctly\n",
    "                        func_instance = func(**eval(tool_call.function.arguments))  # Consider safer alternatives to eval\n",
    "                        output = func_instance.run()\n",
    "\n",
    "                        # Ensure output is a string\n",
    "                        if not isinstance(output, str):\n",
    "                            output = str(output)\n",
    "                    except Exception as e:\n",
    "                        output = f\"Error: {e}\"\n",
    "                else:\n",
    "                    output = \"Function not found\"\n",
    "                wprint(f\"\\033[33m{tool_call.function.name}: {output}\\033[0m\")\n",
    "                tool_outputs.append({\"tool_call_id\": tool_call.id, \"output\": output})\n",
    "\n",
    "            run = client.beta.threads.runs.submit_tool_outputs(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "                tool_outputs=tool_outputs\n",
    "            )\n",
    "        elif run.status == \"failed\":\n",
    "            raise Exception(f\"Run Failed. Error: {run.last_error}\")\n",
    "        else:\n",
    "            messages = client.beta.threads.messages.list(\n",
    "                thread_id=thread.id\n",
    "            )\n",
    "            latest_message = messages.data[0].content[0].text.value\n",
    "            return latest_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_input = \"Give me one course\"\n",
    "# with open(\"user_input.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "#     text_file.write(user_input)\n",
    "\n",
    "# user_questions = [\"What is the best course?\", \"What is the best course for me?\"]\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    # Initialize an empty string to hold extracted text\n",
    "    pdf_text = \"\"\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)  # Efficient way to load a page\n",
    "        pdf_text += page.get_text()\n",
    "    \n",
    "    # Close the PDF document\n",
    "    pdf_document.close()\n",
    "    \n",
    "    # Write the extracted text to a text file\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(pdf_text)\n",
    "\n",
    "# # Usage\n",
    "# pdf_path = \"user_uploads/tarheeltraker.pdf\"  # Path to the PDF file\n",
    "# output_txt_path = \"pdf.txt\"  # Desired output text file path\n",
    "# extract_text_from_pdf(pdf_path, output_txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_content_printer:\n",
    "    openai_schema = {\n",
    "        \"name\": \"pdf_content_printer\",\n",
    "        \"description\": \"Reads and prints the content of 'pdf.txt' file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename=\"pdf.txt\"):\n",
    "        self.filename = filename\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "\n",
    "            if not contents:\n",
    "                return {\"error\": \"The file is empty or does not exist.\"}\n",
    "\n",
    "            print(f\"Contents of {self.filename}:\\n{contents}\")\n",
    "\n",
    "            return {\"output\": f\"Contents of {self.filename} printed successfully.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# # Usage example:\n",
    "# printer = pdf_content_printer()\n",
    "# result = printer.run()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_input_printer:\n",
    "    openai_schema = {\n",
    "        \"name\": \"user_input_printer\",\n",
    "        \"description\": \"Reads and prints the content of 'user_input.txt' file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename=\"user_input.txt\"):\n",
    "        self.filename = filename\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as file:\n",
    "                contents = file.read()\n",
    "\n",
    "            if not contents:\n",
    "                return {\"error\": \"The file is empty or does not exist.\"}\n",
    "\n",
    "            print(f\"Contents of {self.filename}:\\n{contents}\")\n",
    "\n",
    "            return {\"output\": f\"Contents of {self.filename} printed successfully.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# # Usage example:\n",
    "# printer = user_input_printer()\n",
    "# result = printer.run()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 The following text is a Tar Heel Tracker document containing various academic details. \n",
    "#                 Directly extract information relevant to the user's question from the document. \n",
    "#                 Make sure to add necessary context to make the result fully understandable without any \n",
    "#                 external context or help needed. So for example, if you must return a list of requirements, you must\n",
    "#                 include if they are satisfied or unsatisfied. This would make it so that the response is fully \n",
    "#                 understable while being standalone.\n",
    "#                 The requirements are formatted in a way that there are headings for a group of requirements with satsified and unsatisfied \n",
    "#                 requirements below the heading. Sometimes theres only one requirement within the headings. Also a head could say \"Overall Requirement Not Satisfied\"\n",
    "#                 or it could \"Not Satisfied\" just like actual course requirements. It up to you to be able to intelligently figure out which is a heading\n",
    "#                 and which isn't. However if it says \"Overall Requirement Not Satisfied\", that's definetely a heading. Make sure to distinguish between the two in your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_compressor:\n",
    "    openai_schema = {\n",
    "        \"name\": \"pdf_compressor\",\n",
    "        \"description\": \"Takes the pdf text given by the user and compresses to only include relevant information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.openai_key = \"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\"\n",
    "        self.model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    def read_file_content(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                return file.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from {filename}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_prompt(self, pdf_text, user_input):\n",
    "        return f\"Context:\\n{pdf_text}\\n\\nQ: {user_input}\\nA:\"\n",
    "\n",
    "    def generate_system_instruction(self):\n",
    "        return \"\"\"\n",
    "                The following text is a Tar Heel Tracker document containing various academic details. \n",
    "                Directly extract information about what requirements have been met and what requirements are still needed.\n",
    "                Make sure to add necessary context to make the result fully understandable without any \n",
    "                external context or help needed. So when you return a list of requirements, you must\n",
    "                include if they are satisfied or unsatisfied. This would make it so that the response is fully \n",
    "                understable while being standalone.\n",
    "                The requirements are formatted in a way that there are headings for a group of requirements with satsified and unsatisfied \n",
    "                requirements below the heading. Sometimes theres only one requirement within the headings. Also a head could say \"Overall Requirement Not Satisfied\"\n",
    "                or it could \"Not Satisfied\" just like actual course requirements. It up to you to be able to intelligently figure out which is a heading\n",
    "                and which isn't. However if it says \"Overall Requirement Not Satisfied\", that's definetely a heading. Make sure to distinguish between the two in your output.\n",
    "                \"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        client = openai.OpenAI(api_key=self.openai_key)\n",
    "        pdf_text = self.read_file_content(\"pdf.txt\")\n",
    "        user_input = self.read_file_content(\"user_input.txt\")\n",
    "\n",
    "        prompt = self.generate_prompt(pdf_text, user_input)\n",
    "        system_instruct = self.generate_system_instruction()\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"{system_instruct}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            with open(\"pdf_compressed.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(response.choices[0].message.content)\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {str(e)}\")\n",
    "            return \"An error occurred while generating the course schedule advice.\"\n",
    "\n",
    "# # Example usage:\n",
    "# advisor = pdf_compressor()\n",
    "# advice = advisor.run()\n",
    "# print(advice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pdf_content_extractor:\n",
    "    openai_schema = {\n",
    "        \"name\": \"pdf_content_extractor\",\n",
    "        \"description\": \"Takes text as an input and writes it to 'pdf_compressed.txt' file\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The text content to be written to the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, text, filename=\"pdf_compressed.txt\"):\n",
    "        self.filename = filename\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # Ensure text is a string\n",
    "            if not isinstance(self.text, str):\n",
    "                return {\"error\": \"Input text must be a string.\"}\n",
    "\n",
    "            # Write the provided text to the specified file\n",
    "            with open(self.filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(self.text)\n",
    "\n",
    "            return {\"output\": f\"Text has been successfully written to {self.filename}.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# # Example usage:\n",
    "# text_input = \"Your text here.\"  # Replace this with your actual text input\n",
    "# extractor = pdf_content_extractor()\n",
    "# result = extractor.run(text_input)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vector_search_tool:\n",
    "    openai_schema = {\n",
    "        \"name\": \"vector_search_tool\",\n",
    "        \"description\": \"Performs a vector search based on the query in 'user_input.txt' file and returns the most relevant results.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename=\"user_input_modified.txt\"):\n",
    "        # self.weaviate_key = os.environ.get('WEAVIATE_API_KEY')\n",
    "        # self.weaviate_url = os.environ.get('WEAVIATE_URL')\n",
    "        # self.openai_key = os.environ.get('OPENAI_API_KEY')\n",
    "        self.openai_key = \"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\"\n",
    "        self.weaviate_key = \"lPMSnt78RQzC2LNRFEwJWrLUopCzeJ5h9tPi\"\n",
    "        self.weaviate_url = \"https://ramvisor-sbwhtuuh.weaviate.network\"\n",
    "        self.filename = filename\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as file:\n",
    "                user_input = file.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from {self.filename}: {str(e)}\")\n",
    "            return None\n",
    "        if not user_input:\n",
    "            return {\"error\": \"The 'user_input.txt' file is empty or does not exist.\"}\n",
    "\n",
    "        auth_config = weaviate.AuthApiKey(api_key=self.weaviate_key)\n",
    "        \n",
    "        client = weaviate.Client(\n",
    "            url=self.weaviate_url,\n",
    "            auth_client_secret=auth_config,\n",
    "            additional_headers={\n",
    "                \"X-OpenAI-Api-Key\": self.openai_key,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Perform a vector search query to find text chunks related to the input, focusing on headings\n",
    "        nearText = {\n",
    "            \"concepts\": [user_input],\n",
    "            \"properties\": [\"combined_headings\"]  # Focus on the combined headings for the search\n",
    "        }\n",
    "\n",
    "        response = (\n",
    "            client.query\n",
    "            .get(\"TextChunk\", [\"main_heading\", \"subheading\", \"final_subheading\", \"content\"])\n",
    "            # .with_near_text(nearText)\n",
    "            .with_hybrid(\n",
    "                query = user_input,\n",
    "                alpha = 0.8\n",
    "            )\n",
    "            .with_limit(20)  # Adjust the limit as needed to return more results\n",
    "            .with_additional(\"certainty\")  # Request the similarity scores (certainty)\n",
    "            .do()\n",
    "        )\n",
    "\n",
    "        # Process the response to format it as two separate lists: one for text chunks, one for similarity scores\n",
    "        formatted_results = []\n",
    "\n",
    "        for item in response['data']['Get']['TextChunk']:\n",
    "            # Reconstruct the text chunk with headings and content\n",
    "            text_chunk_headings = ''\n",
    "            if item.get('main_heading'):\n",
    "                text_chunk_headings += item['main_heading']\n",
    "            if item.get('subheading'):\n",
    "                text_chunk_headings += ' - ' + item['subheading']\n",
    "            if item.get('final_subheading'):\n",
    "                text_chunk_headings += ' - ' + item['final_subheading']\n",
    "            if item.get('content'):\n",
    "                text_chunk_headings += ':' + item['content']\n",
    "            formatted_results.append(text_chunk_headings)\n",
    "        \n",
    "        keywords = [\"FY-SEMINAR\", \"FY_LAUNCH\", \"GLBL-LANG\", \"FC-AESTH\", \"FC-CREATE\", \"FC-PAST\", \"FC-VALUES\", \"FC-GLOBAL-FC-NATSCI\", \"FC-POWER\", \"FC-QUANT\", \"FC-KNOWING\", \"FC-LAB\"]\n",
    "\n",
    "        # Rerank the results based on specific keywords in the headings\n",
    "        reranked_results = sorted(\n",
    "            formatted_results,\n",
    "            key=lambda x: any(keyword in x.split(':', 1)[0] for keyword in keywords),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # Keep only the top 5 results after reranking\n",
    "        top_results = reranked_results[:5]\n",
    "\n",
    "        matched_results = '\\n\\n'.join(top_results)\n",
    "    \n",
    "        with open(\"vector_search_result.txt\", 'a', encoding='utf-8') as file:\n",
    "            file.write(matched_results)\n",
    "\n",
    "        headings = []\n",
    "        with open(\"vector_search_result.txt\", 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Split the line to extract the heading\n",
    "                heading = line.split(':')[0]  # This gets the text before the colon, which are the headings\n",
    "                headings.append(heading)\n",
    "        headings = [item for item in headings if item != '\\n']\n",
    "        return headings\n",
    "\n",
    "# # Example usage:\n",
    "# tool = vector_search_tool()\n",
    "# result = tool.run()\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_input_modifier:\n",
    "    openai_schema = {\n",
    "        \"name\": \"user_input_modifier\",\n",
    "        \"description\": \"Modifies the user's question to get better vector search results.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The text content to be written to the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, text, filename=\"user_input_modified.txt\"):\n",
    "        self.filename = filename\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # Ensure text is a string\n",
    "            if not isinstance(self.text, str):\n",
    "                return {\"error\": \"Input text must be a string.\"}\n",
    "\n",
    "            # Write the provided text to the specified file\n",
    "            with open(self.filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(self.text)\n",
    "\n",
    "            return {\"output\": f\"Text has been successfully written to {self.filename}.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# # Example usage:\n",
    "# text_input = \"Your text here.\"  # Replace this with your actual text input\n",
    "# extractor = user_input_modifier()\n",
    "# result = extractor.run(text_input)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class course_schedule_advisor:\n",
    "    openai_schema = {\n",
    "        \"name\": \"course_schedule_advisor\",\n",
    "        \"description\": \"Plans a course schedule for the user and returns it to them\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.openai_key = \"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\"\n",
    "        self.model = \"gpt-4-1106-preview\"\n",
    "\n",
    "    def read_file_content(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                return file.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from {filename}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_prompt(self, matched_results, pdf_text_for_input, user_input):\n",
    "        return f\"\"\"\n",
    "                Context Information:\n",
    "                __________\n",
    "                {matched_results}\n",
    "                __________\n",
    "                Context about the user's personal information and cirumstances:\n",
    "                __________\n",
    "                {pdf_text_for_input}\n",
    "                __________\n",
    "\n",
    "                Q: {user_input}. \n",
    "\n",
    "                A:\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_system_instruction(self):\n",
    "        return \"\"\"\n",
    "                When planning a course schedule for any major, it is crucial to integrate the following considerations:\n",
    "\n",
    "                1. Focus Capacities: Every major requires completion of a course in each of nine focus capacities (Aesthetic and Interpretive Analysis, Creative Expression, Engagement with the Human Past, etc.), each worth 3 credit hours. One of these courses must include or be associated with a one-credit lab, specifically the Empirical Investigation Lab.\n",
    "\n",
    "                2. Credit Hour Management: Students must complete 120 credit hours in total to graduate. Each academic year consists of two semesters, with students typically enrolling in 12 to 18 credit hours per semester. Special permission is required to exceed this limit.\n",
    "\n",
    "                3. Detailed Course Analysis: Examine the specific credit hours and requirements for each course, ensuring that every recommended course aligns with the major's requirements and the focus capacities.\n",
    "\n",
    "                4. Semester Balance: Provide a balanced schedule for each semester, taking into account the workload and prerequisites, while ensuring that the focus capacities are evenly distributed throughout the academic years.\n",
    "\n",
    "                5. Graduation Pathway: Create a comprehensive plan that outlines a pathway to graduation, ensuring all focus capacities and major-specific requirements are met within the standard timeframe of the degree.\n",
    "\n",
    "                6. Flexibility for Electives: Include room for elective courses that students may choose based on their interests, ensuring these do not conflict with the completion of required courses.\n",
    "\n",
    "                7. Lab Requirement: Identify which focus capacity course will include the Empirical Investigation Lab, fulfilling the lab requirement.\n",
    "\n",
    "                8. Adjustments for Academic Progress: Adjust the schedule based on the student's year of study, ensuring that the course recommendations are appropriate for their current academic standing.\n",
    "\n",
    "                9. Special Permissions: Acknowledge when special permission might be needed for a student to enroll in more than the standard credit hour limit per semester.\n",
    "\n",
    "                10. Read through the user's personal information so that whatever course schedule you provided doesn't satisy requirements already satsified by the user\n",
    "\n",
    "                In your response, provide a detailed semester-by-semester breakdown of courses, including focus capacity courses, major-specific courses, electives, and the lab requirement. Ensure that the plan adheres to the credit hour constraints per semester and totals to the required 120 credit hours for graduation.\n",
    "                \"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        client = openai.OpenAI(api_key=self.openai_key)\n",
    "        matched_results = self.read_file_content(\"vector_search_result.txt\")\n",
    "        pdf_text_for_input = self.read_file_content(\"pdf_compressed.txt\")\n",
    "        user_input = self.read_file_content(\"user_input.txt\")\n",
    "\n",
    "        prompt = self.generate_prompt(matched_results, pdf_text_for_input, user_input)\n",
    "        system_instruct = self.generate_system_instruction()\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"{system_instruct}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            with open(\"schedule.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(response.choices[0].message.content)\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {str(e)}\")\n",
    "            return \"An error occurred while generating the course schedule advice.\"\n",
    "\n",
    "# # Example usage:\n",
    "# advisor = course_schedule_advisor()\n",
    "# advice = advisor.run()\n",
    "# print(advice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class general_question_advisor:\n",
    "    openai_schema = {\n",
    "        \"name\": \"general_question_advisor\",\n",
    "        \"description\": \"Answers general questions about registration at the Unviersity of North Carolina at Chapel Hill\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.openai_key = \"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\"\n",
    "        self.model = \"gpt-3.5-turbo-1106\"\n",
    "\n",
    "    def read_file_content(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                return file.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from {filename}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def generate_prompt(self, matched_results, pdf_text_for_input, user_input):\n",
    "        return f\"\"\"\n",
    "                Context:\n",
    "                __________\n",
    "                {matched_results}\n",
    "                __________\n",
    "                Context about the user's personal information and cirumstances:\n",
    "                __________\n",
    "                {pdf_text_for_input}\n",
    "                __________\n",
    "                Q: {user_input}\n",
    "\n",
    "                A: \n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "    def generate_system_instruction(self):\n",
    "        return \"\"\"\n",
    "\n",
    "            1. Emphasize Headings: Instruct the AI to pay particular attention to headings that match the keywords, as these are likely to contain the most relevant information.\n",
    "\n",
    "            2. Validate Relevance: After identifying potential answers, the AI should check if the content under the relevant headings actually relates to the query. If not, it should continue the search or notify the user.\n",
    "\n",
    "            3. Admit Uncertainty: If the AI cannot find a clear match or if the content under the relevant headings does not provide a sufficient answer, it should state its uncertainty and suggest alternative ways for the user to find the necessary information, such as contacting an academic advisor or checking the official course catalog.\n",
    "            \n",
    "            4. Identify Keywords: When a question is asked, identify the main keywords, especially course numbers, course names, or specific requirement criteria.\n",
    "\n",
    "            5. Contextual Search: Use these keywords to search through the provided context data. Look for any mention of these keywords within the course descriptions, prerequisites, or degree requirements.\n",
    "\n",
    "\n",
    "            7. Justify with Direct Quotes: Provide direct quotes from the context that pertain to the keywords as justification for the answer you give. If a course number or name is mentioned in relation to specific degree requirements, include this in your response.\n",
    "\n",
    "            8. Clear and Specific Answers: Your response should clearly state whether the course in question meets the specified requirement, based on the context data where the keyword appears. Justify each of your answers individually and state why you gave them.\n",
    "\n",
    "            9. Clarify Uncertainties: If the context does not contain clear information or if the keyword search yields no results, inform the user that the question cannot be answered based on the current data and suggest seeking additional information from official sources.\n",
    "\n",
    "            10. Read through the user's personal information so that whatever response you provide doesn't logically conflict with the user's personal situation\n",
    "\n",
    "            By focusing on these steps, the system should provide responses that are directly linked to the text's mentions of the keywords found in the user's question, ensuring that the answers are data-driven and verifiable.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "    def run(self):\n",
    "        client = openai.OpenAI(api_key=self.openai_key)\n",
    "        matched_results = self.read_file_content(\"vector_search_result.txt\")\n",
    "        pdf_text_for_input = self.read_file_content(\"pdf_compressed.txt\")\n",
    "        user_input = self.read_file_content(\"user_input.txt\")\n",
    "\n",
    "        prompt = self.generate_prompt(matched_results, pdf_text_for_input, user_input)\n",
    "        system_instruct = self.generate_system_instruction()\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"{system_instruct}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            with open(\"general_answer.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "                text_file.write(response.choices[0].message.content)\n",
    "\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {str(e)}\")\n",
    "            return \"An error occurred while generating the course schedule advice.\"\n",
    "\n",
    "# # Example usage:\n",
    "# advisor = general_question_advisor()\n",
    "# advice = advisor.run()\n",
    "# print(advice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class list_printer:\n",
    "    openai_schema = {\n",
    "        \"name\": \"list_printer\",\n",
    "        \"description\": \"Prints out the list of headings\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "            \"required\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, filename1=\"vector_search_result.txt\", filename2=\"user_input.txt\"):\n",
    "        self.filename1 = filename1\n",
    "        self.filename2 = filename2\n",
    "\n",
    "    def run(self):\n",
    "        with open(self.filename2, 'r', encoding='utf-8') as file:\n",
    "            question = file.read()\n",
    "\n",
    "        headings = []\n",
    "        with open(self.filename1, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Split the line to extract the heading\n",
    "                heading = line.split(':')[0]  # This gets the text before the colon, which are the headings\n",
    "                headings.append(heading)\n",
    "        headings = [item for item in headings if item != '\\n']\n",
    "        return f\"User Question: {question} Headings: {headings}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vector_list_modifier:\n",
    "    openai_schema = {\n",
    "        \"name\": \"vector_list_modifier\",\n",
    "        \"description\": \"Modifies the list of headings to leave behind only relevant headings\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The string representation of a list to be converted and written to the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, text, filename=\"relevant_list.txt\"):\n",
    "        self.filename = filename\n",
    "        self.text = text\n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            # Convert the string representation of a list to an actual list\n",
    "            converted_list = ast.literal_eval(self.text)\n",
    "\n",
    "            # Ensure the converted object is a list\n",
    "            if not isinstance(converted_list, list):\n",
    "                return {\"error\": \"Input text must be a string representation of a list.\"}\n",
    "\n",
    "            # Write the converted list to the specified file\n",
    "            with open(self.filename, 'w', encoding='utf-8') as file:\n",
    "                for item in converted_list:\n",
    "                    file.write(f\"{item}\\n\")\n",
    "\n",
    "            return {\"output\": f\"List has been successfully written to {self.filename}.\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     text_input = \"['Economics Major, B.S. (ECON) - Requirements', 'Computer Science Major, B.S. (COMP) - Requirements']\"\n",
    "#     modifier = vector_list_modifier(text=text_input)\n",
    "#     result = modifier.run()\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader_and_modifier_tools = [pdf_content_printer, pdf_compressor]\n",
    "\n",
    "pdf_reader_and_modifier_agent = client_openai.beta.assistants.create(\n",
    "    name='Pdf Reader and Modifier Agent',\n",
    "    instructions= f\"\"\"\n",
    "    You are a Pdf Reader and Modifier Agent. You're job is to read through the user's personal information and return what's most relevant to the user's question.\n",
    "    First use the 'pdf_content_printer' tool to print the user's personal information and cirumstances.\n",
    "    If there exists some print results then take these steps:\n",
    "        Use the 'pdf_compressor' tool to compress the pdf results.\n",
    "        Determine if the result from 'pdf_compressor' is relevant to the user's question. If not rerun 'pdf_compressor' and run the 'pdf_content_printer' to check again for relevance.\n",
    "        Keep this loop going until the result is satisfactory.\n",
    "    Once the result is relevant to the user's question, print \"Compressed Succesfully\" and end the run.\n",
    "    \"\"\",\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    tools=[{\"type\": \"function\", \"function\": pdf_content_printer.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": pdf_compressor.openai_schema},\n",
    "          ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Run the 'vector_search_tool'. This will perform a vector search on a database and return information that should be relevant to answering this question.\n",
    "        # The 'vector_search_tool' will return a resopnse of all the headings of the text chunks that were outputted. \n",
    "        # What's being shown to you are the headings of the paragraphs that are relevant to the user's question. So read the headings and decide if the information those headings would contain is relevant to answering the question.\n",
    "        # There are many circumstances in which the first or second vector search will not have adequate information and/or will have extra unecessary information.\n",
    "        # So it's important that you think deeply about whether or not more vector searches are necessary.\n",
    "        # If you deem that there isn't enough information, then use 'user_input_modifier' to modify the user input to make it more specific and get better results from the search. Then run the vector search again.\n",
    "        # The information to answer the user's question is always in the vector database, so if it's still not returning all relevant information after several searches then use 'user_input_modifier' again and make necessary drastic changes to the original question and input the new question into the tool as a string in the \"text\" paramater.\n",
    "        # Sometimes it might be necessary to break the original question down into components, so that the vector search can work better. The original meaning of the question doesn't have to be maintained as long as the results from the vector search can answer the original question.\n",
    "        # Do not stop repeating until you have adequate information to answer the user's question, but you should have a max of 4 iterations.\n",
    "        # Once the headings indicate to you that there is sufficient information to answer the question, return \"Finished\" and end the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_tools = [vector_search_tool, user_input_modifier]\n",
    "\n",
    "vector_search_agent = client_openai.beta.assistants.create(\n",
    "    name='Vector Search Agent',\n",
    "    instructions= f\"\"\"\n",
    "        Run the 'vector_search_tool'. This will perform a vector search on a database and return information that should be relevant to answering this question.\n",
    "        The 'vector_search_tool' will return a resopnse of all the headings of the text chunks that were outputted. \n",
    "        What's being shown to you are the headings of the paragraphs that are relevant to the user's question. So read the headings and decide if the information those headings would contain is relevant to answering the question.\n",
    "        If you deem that the headings don't provide a comprehensive view of the topic, then use 'user_input_modifier' to modify the user input to make it more specific and get better results from the search. Then run the vector search again.\n",
    "        The information to answer the user's question is always in the vector database, so if it's still not returning all relevant information after several searches then use 'user_input_modifier' again and make necessary drastic changes to the original question and input the new question into the tool as a string in the \"text\" paramater.\n",
    "        Sometimes it might be necessary to break the original question down into components, so that the vector search can work better. The original meaning of the question doesn't have to be maintained as long as the results from the vector search can answer the original question.\n",
    "        Do not stop repeating until you have adequate information to answer the user's question, but you should have a max of 4 iterations.\n",
    "        Once the headings indicate to you that there is sufficient information to answer the question, return \"Finished\" and end the run.\n",
    "    \"\"\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"function\", \"function\": vector_search_tool.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": user_input_modifier.openai_schema},\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_compressor_tools = [list_printer, vector_list_modifier]\n",
    "\n",
    "vector_compressor_agent = client_openai.beta.assistants.create(\n",
    "    name='Vector Compressor Agent',\n",
    "    instructions= f\"\"\"\n",
    "        You are a vector compressor agent. Your job is to take a list of headings and return only the headings that are relevant to the user question.\n",
    "        First use the 'list_printer' tool to print the user question and the list of headings that were returned for the user question.\n",
    "        Now decide which of these headings are actually relevant and necessary to answer the user question.\n",
    "        Formulate your new list of headings in this format:\n",
    "            [\"heading1\", \"heading2\", \"heading3\", etc...]\n",
    "        The list should only contain headings present in the original list.\n",
    "        Now use the 'vector_list_modifier' tool to send the new list of headings to the 'relevant_list.txt' file.\n",
    "        For the 'text' parameter, input the list as a string.\n",
    "        \"\"\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[{\"type\": \"function\", \"function\": list_printer.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": vector_list_modifier.openai_schema},\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_help_tools = [user_input_printer, course_schedule_advisor, general_question_advisor]\n",
    "\n",
    "registration_help_agent = client_openai.beta.assistants.create(\n",
    "    name='Registration Help Agent',\n",
    "    instructions= f\"\"\"\n",
    "        You are a Registration Help Agent. You're job is to help students with course registration questions and answer any questions they have about UNC registration.\n",
    "        Use the 'user_input_printer' tool to see the question asked by the user. Use this criteria to judge whether the question is about scheduling or not:\n",
    "            Determine if the following question is specifically about planning a course schedule, such as choosing courses for a semester to meet academic requirements, or if it's a general question about course registration that does not involve planning a schedule.\n",
    "\n",
    "        If \"general registration\",\n",
    "            then use the 'general_question_advisor' tool to return a good response to the user for their question that's about registration in general.\n",
    "    \n",
    "        If \"schedule planning\",\n",
    "            then use the 'course_schedule_advisor' tool to return a good response to the user for a sample schedule.\n",
    "        \n",
    "        Under no cirumstances should you use the 'course_schedule_advisor' tool and the 'general_question_advisor' tool in the same question.\n",
    "        Read the responses generated by the tools and decide if the information is relevant and comprehensive enough to answer the user's question.\n",
    "        If not the run the same tool you used intially, whether it was the 'general_question_advisor' or the 'course_schedule_advisor', again and try to get a better result.\n",
    "        Once you're satisfied with the response given, return \"Finished\" to the user and end the run.\n",
    "    \"\"\",\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    tools=[\n",
    "\n",
    "           {\"type\": \"function\", \"function\": user_input_printer.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": course_schedule_advisor.openai_schema},\n",
    "           {\"type\": \"function\", \"function\": general_question_advisor.openai_schema},\n",
    "          \n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_specific_lines(filename, line_indices):\n",
    "    # Read the content of the file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Keep all lines except the ones at the specified indices\n",
    "    # Note: Adjusting for zero-based indexing used in Python\n",
    "    cleaned_lines = [line for idx, line in enumerate(lines) if idx not in [i - 1 for i in line_indices]]\n",
    "    \n",
    "    # Write the updated lines back to the file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_deduplicate_vector_search_results(relevant_headings_file, vector_search_file):\n",
    "    # Read and normalize the relevant headings\n",
    "    with open(relevant_headings_file, 'r', encoding='utf-8') as file:\n",
    "        relevant_headings = {line.strip() for line in file if line.strip()}\n",
    "\n",
    "    # Initialize a set to keep track of headings that have been processed\n",
    "    processed_headings = set()\n",
    "\n",
    "    # Initialize a list to hold the lines to keep, ensuring no duplicate headings\n",
    "    filtered_lines = []\n",
    "\n",
    "    # Process the vector search results for filtering and deduplication\n",
    "    with open(vector_search_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            heading = line.split(':')[0].strip()\n",
    "            if heading in relevant_headings and heading not in processed_headings:\n",
    "                filtered_lines.append(line)\n",
    "                processed_headings.add(heading)\n",
    "\n",
    "    # Overwrite the vector_search_file with the deduplicated and filtered lines\n",
    "    with open(vector_search_file, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(filtered_lines)\n",
    "\n",
    "    print(f\"The {vector_search_file} has been successfully updated with filtered and deduplicated results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_entries(vector_search_file):\n",
    "    # Initialize a set to track headings already encountered\n",
    "    seen_headings = set()\n",
    "    \n",
    "    # Initialize a list to store lines without duplicates\n",
    "    unique_lines = []\n",
    "    \n",
    "    # Read the file and filter out duplicate headings\n",
    "    with open(vector_search_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Extract the heading (text before the colon)\n",
    "            heading = line.split(':')[0].strip()\n",
    "            \n",
    "            # Check if heading has been encountered; if not, add to seen and keep the line\n",
    "            if heading not in seen_headings:\n",
    "                seen_headings.add(heading)\n",
    "                unique_lines.append(line)\n",
    "    \n",
    "    # Overwrite the file with lines that have unique headings\n",
    "    with open(vector_search_file, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(unique_lines)\n",
    "\n",
    "    print(f\"Duplicates have been successfully removed from {vector_search_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file 'combined_course_info.txt' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_course_codes(file_path):\n",
    "    # Regular expression to match course codes:\n",
    "    # Looks for sequences of uppercase letters (at least 2 to include department codes) followed by spaces/digits.\n",
    "    course_code_pattern = re.compile(r'\\b[A-Z]{2,}\\s*\\d{3,}\\b')\n",
    "\n",
    "    # Read the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Replace non-breaking spaces with regular spaces\n",
    "    content = content.replace('\\xa0', ' ')\n",
    "\n",
    "    # Find all matches of the course code pattern in the file content\n",
    "    course_codes = course_code_pattern.findall(content)\n",
    "\n",
    "    # Remove duplicates and sort the list\n",
    "    course_codes = sorted(set(course_codes))\n",
    "\n",
    "    return course_codes\n",
    "\n",
    "def parse_course_info(file_path):\n",
    "    # Initialize a defaultdict to hold lists of course info strings keyed by course code\n",
    "    course_info_dict = defaultdict(list)\n",
    "    \n",
    "    # Regular expression to match the course code and the rest of the line\n",
    "    # This regex now captures the department code and course number separately to normalize spaces later\n",
    "    line_pattern = re.compile(r'^(\\w+)\\s+(\\d+)\\s+(.*)$')\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            match = line_pattern.match(line.strip())\n",
    "            if match:\n",
    "                # Extract department code, course number, and the rest of the info\n",
    "                dept_code, course_number, info = match.groups()\n",
    "                # Normalize spacing between department code and course number\n",
    "                course_code = f\"{dept_code} {course_number}\"\n",
    "                # Append the info to the list associated with the normalized course code in the dict\n",
    "                course_info_dict[course_code].append(info)\n",
    "    \n",
    "    # Convert defaultdict to a regular dict for the final output\n",
    "    return dict(course_info_dict)\n",
    "\n",
    "def remove_common_suffix_from_details(course_details):\n",
    "    # Identify common suffix\n",
    "    common_suffix = None\n",
    "    for detail in course_details:\n",
    "        if common_suffix is None:\n",
    "            common_suffix = detail\n",
    "        else:\n",
    "            # Find the common suffix between the current common suffix and this detail\n",
    "            min_length = min(len(common_suffix), len(detail))\n",
    "            for i in range(min_length):\n",
    "                if common_suffix[-(i+1)] != detail[-(i+1)]:\n",
    "                    common_suffix = common_suffix[-i:]\n",
    "                    break\n",
    "    \n",
    "    # Remove the common suffix from all but the first entry, if any common suffix was found\n",
    "    if common_suffix and len(course_details) > 1:\n",
    "        trimmed_details = [course_details[0]]  # Keep the first entry intact\n",
    "        for detail in course_details[1:]:\n",
    "            if detail.endswith(common_suffix):\n",
    "                # Remove the common suffix from this detail\n",
    "                detail = detail[:-len(common_suffix)].strip()\n",
    "            trimmed_details.append(detail)\n",
    "        return trimmed_details\n",
    "    else:\n",
    "        return course_details\n",
    "\n",
    "\n",
    "\n",
    "def create_combined_file(source_file_path, filtered_dict, output_file_path):\n",
    "    with open(source_file_path, 'r') as source_file, open(output_file_path, 'w') as output_file:\n",
    "        # Write the original content of the source file to the output file\n",
    "        original_content = source_file.read()\n",
    "        output_file.write(original_content + \"\\n\\n\")\n",
    "        \n",
    "        # Write the header for the detailed course info section\n",
    "        output_file.write(\"Detailed Course Info:\\n\\n\")\n",
    "        \n",
    "        # Iterate through the filtered dictionary and write each course's detailed info\n",
    "        for course_code, details in filtered_dict.items():\n",
    "            output_file.write(f\"{course_code}:\\n\")\n",
    "            for detail in details:\n",
    "                output_file.write(f\"  {detail}\\n\")\n",
    "            output_file.write(\"\\n\")  # Add an extra newline for spacing between courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Calling function vector_search_tool\n",
      "\u001b[31mFunction: vector_search_tool\u001b[0m\n",
      "\u001b[33mvector_search_tool: ['Economics Major, B.S. (ECON) -\n",
      "Requirements', 'Economics Major, B.S. (ECON) - Sample Plan of Study',\n",
      "'Computer Science Major, B.S. (COMP) - Sample Plan of Study',\n",
      "'Statistics and Analytics Major, B.S. (STOR) - Requirements - Group\n",
      "B', 'Economics Major, B.S. (ECON) - Requirements', 'Economics Major,\n",
      "B.A. (ECON) - Sample Plan 1', 'Department of Economics (ECON) (BS, BA,\n",
      "Minor) - ECON–Economics', 'Economics Major, B.S. (ECON) - Sample Plan\n",
      "of Study', 'Economics Major, B.S. (ECON) - Requirements', 'Economics\n",
      "Major, B.A. (ECON) - Sample Plan 1', 'Department of Economics (ECON)\n",
      "(BS, BA, Minor) - ECON–Economics', 'Economics Major, B.S. (ECON) -\n",
      "Sample Plan of Study']\u001b[0m\n",
      "Debug: Calling function user_input_modifier\n",
      "\u001b[31mFunction: user_input_modifier\u001b[0m\n",
      "\u001b[33muser_input_modifier: {'output': 'Text has been successfully\n",
      "written to user_input_modified.txt.'}\u001b[0m\n",
      "Debug: Calling function vector_search_tool\n",
      "\u001b[31mFunction: vector_search_tool\u001b[0m\n",
      "\u001b[33mvector_search_tool: ['Economics Major, B.S. (ECON) -\n",
      "Requirements', 'Economics Major, B.S. (ECON) - Sample Plan of Study',\n",
      "'Computer Science Major, B.S. (COMP) - Sample Plan of Study',\n",
      "'Statistics and Analytics Major, B.S. (STOR) - Requirements - Group\n",
      "B', 'Economics Major, B.S. (ECON) - Requirements', 'Economics Major,\n",
      "B.A. (ECON) - Sample Plan 1', 'Department of Economics (ECON) (BS, BA,\n",
      "Minor) - ECON–Economics', 'Economics Major, B.S. (ECON) - Sample Plan\n",
      "of Study', 'Economics Major, B.S. (ECON) - Requirements', 'Economics\n",
      "Major, B.A. (ECON) - Sample Plan 1', 'Department of Economics (ECON)\n",
      "(BS, BA, Minor) - ECON–Economics', 'Economics Major, B.S. (ECON) -\n",
      "Sample Plan of Study', 'Economics Major, B.S. (ECON) - Sample Plan of\n",
      "Study', 'Economics Major, B.A. (ECON) - Sample Plan 1', 'Economics\n",
      "Major, B.A. (ECON) - Sample Plan 2 (Honors Thesis Track)', 'Economics\n",
      "Major, B.S. (ECON) - Department Programs']\u001b[0m\n",
      "\u001b[34mVector Search Agent: Finished\u001b[0m\n",
      "Duplicates have been successfully removed from vector_search_result.txt.\n",
      "Debug: Calling function list_printer\n",
      "\u001b[31mFunction: list_printer\u001b[0m\n",
      "\u001b[33mlist_printer: User Question: Give me a full schedule for my Econ\n",
      "BS and Comp Sci BS double major with nothing missing Headings:\n",
      "['Economics Major, B.S. (ECON) - Requirements', 'Economics Major, B.S.\n",
      "(ECON) - Sample Plan of Study', 'Computer Science Major, B.S. (COMP) -\n",
      "Sample Plan of Study', 'Statistics and Analytics Major, B.S. (STOR) -\n",
      "Requirements - Group B', 'Economics Major, B.A. (ECON) - Sample Plan\n",
      "1', 'Department of Economics (ECON) (BS, BA, Minor) - ECON–Economics',\n",
      "'Economics Major, B.A. (ECON) - Sample Plan 2 (Honors Thesis Track)',\n",
      "'Economics Major, B.S. (ECON) - Department Programs']\u001b[0m\n",
      "Debug: Calling function vector_list_modifier\n",
      "\u001b[31mFunction: vector_list_modifier\u001b[0m\n",
      "\u001b[33mvector_list_modifier: {'error': \"An error occurred: invalid\n",
      "character '–' (U+2013) (<unknown>, line 1)\"}\u001b[0m\n",
      "Debug: Calling function vector_list_modifier\n",
      "\u001b[31mFunction: vector_list_modifier\u001b[0m\n",
      "\u001b[33mvector_list_modifier: {'error': 'An error occurred: invalid\n",
      "syntax (<unknown>, line 1)'}\u001b[0m\n",
      "Debug: Calling function vector_list_modifier\n",
      "\u001b[31mFunction: vector_list_modifier\u001b[0m\n",
      "\u001b[33mvector_list_modifier: {'output': 'List has been successfully\n",
      "written to relevant_list.txt.'}\u001b[0m\n",
      "\u001b[34mVector Compressor Agent: The relevant headings have been\n",
      "successfully modified.\u001b[0m\n",
      "The vector_search_result.txt has been successfully updated with filtered and deduplicated results.\n",
      "Debug: Calling function user_input_printer\n",
      "\u001b[31mFunction: user_input_printer\u001b[0m\n",
      "Contents of user_input.txt:\n",
      "Give me a full schedule for my Econ BS and Comp Sci BS double major with nothing missing\n",
      "\u001b[33muser_input_printer: {'output': 'Contents of user_input.txt\n",
      "printed successfully.'}\u001b[0m\n",
      "Debug: Calling function course_schedule_advisor\n",
      "\u001b[31mFunction: course_schedule_advisor\u001b[0m\n",
      "\u001b[33mcourse_schedule_advisor: Given your request for a full schedule\n",
      "for an Economics (BS) and Computer Science (BS) double major, and\n",
      "considering the detailed course information provided, I will create a\n",
      "comprehensive plan for both majors.  Assuming you are starting as a\n",
      "first-year student and have not completed any courses, your schedule\n",
      "will look something like this:  **Freshman Year** - **Fall Semester\n",
      "(17 credit hours)**   1. ENGL 105 - ENG COMP & RHETORIC (3 credit\n",
      "hours) [CR]   2. MATH 231 - CALC FUNC ONE VAR I (4 credit hours) [QR]\n",
      "3. ECON 101 - ECON: INTRO (4 credit hours) [SS]   4. COMP 110 - INTRO\n",
      "PROGRAMMING (3 credit hours) [QR]   5. IDST 101 - COLLEGE THRIVING (1\n",
      "credit hour)   6. Elective / Focus Capacity Course (2 credit hours)  -\n",
      "**Spring Semester (17 credit hours)**   1. MATH 232 - CAL FUNC ONE VAR\n",
      "II (4 credit hours) [QI]   2. STOR 215 - INTRO TO DATA MODELS (3\n",
      "credit hours)   3. COMP 210 - ANALYSIS OF DATA STRUCTURE (3 credit\n",
      "hours)   4. ECON 400 - DATA SCIENCE AND ECONOMETRICS (4 credit hours)\n",
      "[QI]   5. Focus Capacity Course (3 credit hours)  **Sophomore Year** -\n",
      "**Fall Semester (17 credit hours)**   1. MATH 233 - MULTI VARI CALC I\n",
      "(4 credit hours) [QI]   2. COMP 283 - DISCRETE STRUCTURES (3 credit\n",
      "hours) [QR]   3. COMP 301 - STRUCTURE/ORGANIZE OWN CODE (3 credit\n",
      "hours)   4. Focus Capacity Course (3 credit hours)   5. Elective /\n",
      "Minor Requirement (3 credit hours)   6. ECON 410 - INTERMEDIATE\n",
      "MICROECONOMICS (4 credit hours)  - **Spring Semester (17 credit\n",
      "hours)**   1. STOR 435 - INTRO TO PROBABIL (3 credit hours) [QI]   2.\n",
      "Focus Capacity Course with Empirical Investigation Lab (4 credit\n",
      "hours) [Including Lab Requirement]   3. COMP 211 - SYSTEM PROGRAMMING\n",
      "FUNDAMENTALS (3 credit hours)   4. ECON 420 - INTERMEDIATE\n",
      "MACROECONOMICS (3 credit hours)   5. Elective / Minor Requirement (3\n",
      "credit hours)  **Junior Year** - **Fall Semester (17 credit hours)**\n",
      "1. STOR 415 - INTRO TO OPTIMIZATION (3 credit hours) [QI]   2. COMP\n",
      "550 - ALGRTHMS & ANALYSIS (3 credit hours) [QR]   3. ECON 470 -\n",
      "ECONOMETRICS (3 credit hours) [QI]   4. Focus Capacity Course (3\n",
      "credit hours)   5. Elective / Minor Requirement (3 credit hours)   6.\n",
      "Physical Education Course (1 credit hour)  - **Spring Semester (17\n",
      "credit hours)**   1. COMP 455 - MODELS: LANGS/COMPN (3 credit hours)\n",
      "2. COMP 311 - COMPUTER ORGANIZATION (3 credit hours)   3. MATH 381 -\n",
      "DISCRETE MATH (3 credit hours) [QI]   4. ECON 384 - GATEWAY TO\n",
      "PHIL/POLI/ECON (3 credit hours) [PH]   5. Focus Capacity Course (3\n",
      "credit hours)   6. Elective / Minor Requirement (2 credit hours)\n",
      "**Senior Year** - **Fall Semester (15 credit hours)**   1. ECON 698 -\n",
      "PHIL/POLI/ECON CAPSTONE (3 credit hours) [Research Intensive]   2.\n",
      "COMP 410 - SOFTWARE SYSTEMS (3 credit hours)   3. Upper Level ECON\n",
      "Elective (3 credit hours)   4. Elective (3 credit hours)   5. Focus\n",
      "Capacity Course (3 credit hours)    - **Spring Semester (12-15 credit\n",
      "hours)**   1. Upper Level COMP Electives (6 credit hours)   2. Upper\n",
      "Level ECON Electives (6 credit hours)   3. Elective (3 credit hours)\n",
      "[optional]  Remember that:  - You should check for prerequisites and\n",
      "course offerings for each semester. - You need a total of 120 credit\n",
      "hours to graduate, with appropriate distribution of focus capacity\n",
      "courses. - Electives and focus capacity courses should be chosen based\n",
      "on the individual's interests. - This schedule assumes no Advanced\n",
      "Placement (AP) credits or transfer credits. - The mix of classes\n",
      "manages workload by balancing core major, focus capacity, and elective\n",
      "courses each semester.  Please consult your academic advisor for a\n",
      "personalized schedule that takes into account your current academic\n",
      "standing, any AP credits, or personal preferences.\u001b[0m\n",
      "Debug: Calling function general_question_advisor\n",
      "\u001b[31mFunction: general_question_advisor\u001b[0m\n",
      "Error calling OpenAI API: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 30370 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "\u001b[33mgeneral_question_advisor: An error occurred while generating the\n",
      "course schedule advice.\u001b[0m\n",
      "Debug: Calling function course_schedule_advisor\n",
      "\u001b[31mFunction: course_schedule_advisor\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rithvikprakki/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/convertoassistant.ipynb Cell 25\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m create_combined_file(source_file_path, filtered_course_info, output_file_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m thread \u001b[39m=\u001b[39m client_openai\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mcreate()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m message \u001b[39m=\u001b[39m get_completion(user_input, registration_help_agent, registration_help_tools, thread, client_openai)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m wprint(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[34m\u001b[39m\u001b[39m{\u001b[39;00mregistration_help_agent\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvector_search_result.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m text_file:\n",
      "\u001b[1;32m/Users/rithvikprakki/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/convertoassistant.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Assuming arguments are parsed correctly\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     func_instance \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39meval\u001b[39m(tool_call\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39marguments))  \u001b[39m# Consider safer alternatives to eval\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     output \u001b[39m=\u001b[39m func_instance\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# Ensure output is a string\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(output, \u001b[39mstr\u001b[39m):\n",
      "\u001b[1;32m/Users/rithvikprakki/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/convertoassistant.ipynb Cell 25\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m system_instruct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_system_instruction()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         messages\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00msystem_instruct\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m             {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mprompt\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mschedule.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m text_file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rithvikprakki/Desktop/UNC%20Bot%20Data/UNC%20Bot%20Data%202023-2024/convertoassistant.ipynb#X26sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         text_file\u001b[39m.\u001b[39mwrite(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    660\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    661\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    662\u001b[0m             {\n\u001b[1;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    669\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    670\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    671\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    672\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    673\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    674\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    675\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    676\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    677\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    678\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    679\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    680\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    681\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    682\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    683\u001b[0m             },\n\u001b[1;32m    684\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    685\u001b[0m         ),\n\u001b[1;32m    686\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    687\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    688\u001b[0m         ),\n\u001b[1;32m    689\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    690\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    691\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    692\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    870\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    871\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    872\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    873\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    874\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    875\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/openai/_base_client.py:898\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mauth\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_auth\n\u001b[1;32m    897\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    899\u001b[0m         request,\n\u001b[1;32m    900\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    901\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    902\u001b[0m     )\n\u001b[1;32m    903\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    904\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered httpx.TimeoutException\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    916\u001b[0m     request,\n\u001b[1;32m    917\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    918\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    919\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    944\u001b[0m         request,\n\u001b[1;32m    945\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    946\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1018\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Desktop/UNC Bot Data/UNC Bot Data 2023-2024/ramvisor/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1170\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_input = \"Give me a full schedule for my Econ BS and Comp Sci BS double major with nothing missing\"\n",
    "\n",
    "with open(\"user_input.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(user_input)\n",
    "with open(\"user_input_modified.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(user_input)\n",
    "\n",
    "# pdf_path = \"user_uploads/tarheeltraker.pdf\"  # Path to the PDF file\n",
    "# output_txt_path = \"pdf.txt\"  # Desired output text file path\n",
    "# extract_text_from_pdf(pdf_path, output_txt_path)\n",
    "# lines_to_remove = [2, 3]\n",
    "# remove_specific_lines(\"pdf.txt\", lines_to_remove)\n",
    "\n",
    "file_path = 'pdf.txt'\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "if contents.strip():\n",
    "    thread = client_openai.beta.threads.create()\n",
    "    message = get_completion(user_input, pdf_reader_and_modifier_agent, pdf_reader_and_modifier_tools, thread, client_openai)\n",
    "    wprint(f\"\\033[34m{pdf_reader_and_modifier_agent.name}: {message}\\033[0m\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "thread = client_openai.beta.threads.create()\n",
    "message = get_completion(user_input, vector_search_agent, vector_search_tools, thread, client_openai)\n",
    "wprint(f\"\\033[34m{vector_search_agent.name}: {message}\\033[0m\")\n",
    "\n",
    "vector_search_file = \"vector_search_result.txt\"\n",
    "remove_duplicate_entries(vector_search_file)\n",
    "\n",
    "thread = client_openai.beta.threads.create()\n",
    "message = get_completion(user_input, vector_compressor_agent, vector_compressor_tools, thread, client_openai)\n",
    "wprint(f\"\\033[34m{vector_compressor_agent.name}: {message}\\033[0m\")\n",
    "\n",
    "relevant_headings_file = \"relevant_list.txt\"\n",
    "vector_search_file = \"vector_search_result.txt\"\n",
    "filter_and_deduplicate_vector_search_results(relevant_headings_file, vector_search_file)\n",
    "\n",
    "\n",
    "\n",
    "file_path = 'vector_search_result.txt'\n",
    "course_codes = extract_course_codes(file_path)\n",
    "file_path = 'formatted_classes_info.txt'\n",
    "course_info = parse_course_info(file_path)\n",
    "vector_search_file_path = 'vector_search_result.txt'\n",
    "course_codes = extract_course_codes(vector_search_file_path)\n",
    "# Step 2: Create a new dictionary for the filtered course info\n",
    "filtered_course_info = {}\n",
    "\n",
    "# Use the course_info dictionary generated from 'formatted_classes_info.txt'\n",
    "for code in course_codes:\n",
    "    for key in course_info.keys():\n",
    "        if key.startswith(code):\n",
    "            filtered_course_info[key] = course_info[key]\n",
    "\n",
    "# Apply the function to each set of course details in the filtered_course_info dictionary\n",
    "for course_code, details in filtered_course_info.items():\n",
    "    filtered_course_info[course_code] = remove_common_suffix_from_details(details)\n",
    "\n",
    "# Assuming 'vector_search_result.txt' is your source file and 'filtered_course_info' is your filtered dictionary\n",
    "source_file_path = 'vector_search_result.txt'\n",
    "output_file_path = 'vector_search_result.txt'\n",
    "\n",
    "# Call the function with the path to the source file, the filtered dictionary, and the desired output file path\n",
    "create_combined_file(source_file_path, filtered_course_info, output_file_path)\n",
    "\n",
    "\n",
    "thread = client_openai.beta.threads.create()\n",
    "message = get_completion(user_input, registration_help_agent, registration_help_tools, thread, client_openai)\n",
    "wprint(f\"\\033[34m{registration_help_agent.name}: {message}\\033[0m\")\n",
    "\n",
    "with open(\"vector_search_result.txt\", \"w\", encoding=\"utf-8\") as text_file:\n",
    "    text_file.write(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ramvisor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
