{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input  = \"i am a sophomore studying the computer science BS program. i have finished all math requirements other than multivariable calc, and am finshing comp 301 right now, in my fall semester. next semester im taking comp 211, comp 455, and math 233. i want to graduate the end of my junior year. ive already finished all my gen ed classes. plan out my junior fall and junior spring classes to graduate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What are the 3 best courses numbered comp 420 or greater\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What about the best courses below 400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"above 400\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sophomore\", \"Computer Science\", \"BS\", \"Math\", \"Multivariable Calc\", \"Comp 301\", \"Comp 211\", \"Comp 455\", \"Math 233\", \"Junior\", \"Fall\", \"Spring\", \"Graduate\", \"Gen Ed\", \"Major\", \"Minor\"]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Question: {user_input}\n",
    "\n",
    "\"\"\"\n",
    "system_instruct = \"\"\"\n",
    "You are tasked with identifying keywords from a given question. Ensure the first letter of each keyword is capitalized. \n",
    "If a keyword matches any of the following special terms: \n",
    "FY-SEMINAR, FY-LAUNCH, FY-TRIPLE, GLBL-LANG, FC-AESTH, FC-CREATE, FC-PAST, FC-VALUES, FC-GLOBAL, FC-NATSCI, FC-POWER, FC-QUANT, FC-KNOWING, FC-LAB,\n",
    "then capitalize all letters of the keyword.\n",
    "\n",
    "Additionally, if a keyword contains a hyphen, consider the part after the hyphen as a separate keyword and include it in the list of keywords.\n",
    "If the input question contains the words major or minor include those in the keywords.\n",
    "\n",
    "Format the keywords in a Python list and return it.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_response(system_message, user_prompt):\n",
    "    # Combine system message and user prompt into the messages parameter\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Make the API call\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # Extract and return the model's response\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Get and print the response\n",
    "keywords = get_response(system_instruct, prompt)\n",
    "print(keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Assume keywords_str is your keywords variable as a string\n",
    "keywords_str = keywords\n",
    "\n",
    "# Convert string to list\n",
    "keywords_list = ast.literal_eval(keywords_str)\n",
    "\n",
    "def filter_keywords(keywords):\n",
    "    # List of generic or unnecessary words, all in uppercase for consistent comparison\n",
    "    generic_words = set(word.upper() for word in [\n",
    "        'best', 'worst', 'most', 'least', 'all', 'any', 'very', \n",
    "        'not', 'and', 'or', 'but', 'if', 'else', 'the', 'a', 'an',\n",
    "        'courses', 'schedule', 'major', 'minor'\n",
    "    ])\n",
    "    \n",
    "    # Filter out generic words from the keywords list and add keywords after hyphens\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Exclude generic words\n",
    "        if keyword.upper() not in generic_words:\n",
    "            filtered_keywords.append(keyword)\n",
    "            # Check for hyphen and add keyword after hyphen\n",
    "            if '-' in keyword:\n",
    "                part_after_hyphen = keyword.split('-')[1]\n",
    "                filtered_keywords.append(part_after_hyphen)\n",
    "    \n",
    "    return filtered_keywords\n",
    "\n",
    "# Get and print the filtered keywords\n",
    "filtered_keywords = filter_keywords(keywords_list)\n",
    "print(filtered_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_schedule_planning_question(question):\n",
    "    # Define a prompt that describes the task to the model\n",
    "    prompt = f\"\"\"Classify the following question as either \"course planning/schedule creating\" or \"general registration question\":\n",
    "\n",
    "    Question: \"{question}\"\n",
    "\n",
    "    Classification:\"\"\"\n",
    "\n",
    "    # Send the prompt to the OpenAI API\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=10  # Adjust tokens as needed\n",
    "    )\n",
    "\n",
    "    # Interpret the response\n",
    "    classification = response.choices[0].text.strip().lower()\n",
    "    \n",
    "    # Check if the classification is related to schedule planning\n",
    "    if \"course planning\" in classification or \"schedule creating\" in classification:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_schedule_planning_question(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for when to consider a question as a follow-up\n",
    "SIMILARITY_THRESHOLD = 0.7  # Adjust this threshold based on testing and desired sensitivity\n",
    "def is_follow_up(question, previous_questions):\n",
    "    if not previous_questions:  # If there are no previous questions, return False\n",
    "        return False\n",
    "    all_questions = previous_questions + [question]\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_questions)\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    max_similarity = max(cosine_similarities[0])\n",
    "    return max_similarity >= SIMILARITY_THRESHOLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_follow_up(new_question, previous_questions):\n",
    "    if not previous_questions:  # If there are no previous questions, return False\n",
    "        return False\n",
    "\n",
    "    # Create a prompt for the model\n",
    "    prompt = \"I will provide a series of questions. Determine if the last question is contextually related or a follow-up to any of the previous ones.\\n\\n\"\n",
    "    for i, question in enumerate(previous_questions, 1):\n",
    "        prompt += f\"Q{i}: {question}\\n\"\n",
    "    prompt += f\"New Question: {new_question}\\nIs the New Question a follow-up?\"\n",
    "\n",
    "    # Send the prompt to the OpenAI API\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  # or the latest available engine\n",
    "        prompt=prompt,\n",
    "        max_tokens=50  # Adjust tokens as needed\n",
    "    )\n",
    "\n",
    "    # Interpret the response\n",
    "    answer = response.choices[0].text.strip().lower()\n",
    "    return \"yes\" in answer  # Assuming the model responds with a simple yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_question(user_input):\n",
    "    # # Use regular expressions to replace non-word characters with spaces\n",
    "    # # This will help in splitting the string into words\n",
    "    # words_str = re.sub(r'\\W+', ' ', question_str)\n",
    "    \n",
    "    # # Now split the string into a list of words\n",
    "    # words_list = words_str.split()\n",
    "    \n",
    "    # # Return the list of words\n",
    "    # return words_list\n",
    "    prompt = f\"\"\"\n",
    "    Question: {user_input}\n",
    "\n",
    "    \"\"\"\n",
    "    system_instruct = \"\"\"\n",
    "    You are tasked with identifying keywords from a given question. Ensure the first letter of each keyword is capitalized. \n",
    "    If a keyword matches any of the following special terms: \n",
    "    FY-SEMINAR, FY-LAUNCH, FY-TRIPLE, GLBL-LANG, FC-AESTH, FC-CREATE, FC-PAST, FC-VALUES, FC-GLOBAL, FC-NATSCI, FC-POWER, FC-QUANT, FC-KNOWING, FC-LAB,\n",
    "    then capitalize all letters of the keyword.\n",
    "\n",
    "    Additionally, if a keyword contains a hyphen, consider the part after the hyphen as a separate keyword and include it in the list of keywords.\n",
    "    If the input question contains the words major or minor include those in the keywords.\n",
    "\n",
    "    Format the keywords in a Python list and return it.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def get_response(system_message, user_prompt):\n",
    "        # Combine system message and user prompt into the messages parameter\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        \n",
    "        # Make the API call\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Extract and return the model's response\n",
    "        return response['choices'][0]['message']['content']\n",
    "    \n",
    "    keywords = get_response(system_instruct, prompt)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def filter_keywords(keywords):\n",
    "    # List of generic or unnecessary words, all in uppercase for consistent comparison\n",
    "    generic_words = set(word.upper() for word in [\n",
    "        'and', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'although', 'always', 'among',\n",
    "        'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but',\n",
    "        'came', 'can', 'cannot', 'come', 'could',\n",
    "        'did', 'does', 'doing', 'done', 'down', 'during',\n",
    "        'each', 'either', 'enough', 'even', 'ever', 'every', 'except',\n",
    "        'few', 'from', 'further',\n",
    "        'had', 'has', 'have', 'having', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'into',\n",
    "        'just',\n",
    "        'like', 'likely', 'little',\n",
    "        'made', 'make', 'many', 'may', 'might', 'more', 'most', 'much', 'must',\n",
    "        'never', 'none', 'nor', 'not', 'nothing', 'now',\n",
    "        'only', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over',\n",
    "        'perhaps', 'possible', 'probably', 'perhaps',\n",
    "        'quite',\n",
    "        'rather', 'really', 'regarding', 'right',\n",
    "        'said', 'same', 'seem', 'seemed', 'seeming', 'seems', 'shall', 'should', 'since', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'such',\n",
    "        'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'through', 'thus', 'together', 'too', 'toward', 'towards',\n",
    "        'under', 'until', 'upon', 'used', 'using',\n",
    "        'various', 'very',\n",
    "        'was', 'were', 'what', 'when', 'where', 'which', 'while', 'whom', 'will', 'with', 'within', 'without', 'would',\n",
    "        'your', 'yours', 'yourself', 'yourselves', 'end', 'ive'\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # Filter out generic words from the keywords list and add keywords after hyphens and spaces\n",
    "    # Also remove words that are two characters or less\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Exclude generic words and words that are two characters or less\n",
    "        if keyword.upper() not in generic_words and len(keyword) > 2:\n",
    "            # Capitalize the first letter of each word\n",
    "            filtered_keywords.append(keyword.title())\n",
    "            \n",
    "            # Check for hyphen and add keyword after hyphen\n",
    "            if '-' in keyword:\n",
    "                part_after_hyphen = keyword.split('-')[1]\n",
    "                if len(part_after_hyphen) > 2:  # Also check the length for the part after the hyphen\n",
    "                    filtered_keywords.append(part_after_hyphen.title())\n",
    "            \n",
    "            # Check for space and add separate words\n",
    "            if ' ' in keyword:\n",
    "                parts_with_spaces = keyword.split()\n",
    "                for part in parts_with_spaces:\n",
    "                    if len(part) > 2:  # Check the length for each part\n",
    "                        filtered_keywords.append(part.title())\n",
    "    \n",
    "    return filtered_keywords\n",
    "\n",
    "# initial_keywords = get_keywords_from_question(user_input)\n",
    "# filtered_keywords = filter_keywords(initial_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class HierarchicalChunkProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def load_from_json_files(self, json_paths):\n",
    "        for json_path in json_paths:\n",
    "            with open(json_path, 'r') as file:\n",
    "                data_chunk = json.load(file)\n",
    "                self.data.update(data_chunk)\n",
    "                print(f\"Loaded data from {json_path}. Current number of keys in data: {len(self.data)}\")\n",
    "\n",
    "    def format_chunks(self):\n",
    "        results = []\n",
    "        for heading, sub_content_list in self.data.items():\n",
    "            # Process each item in the sub_content_list\n",
    "            for item in sub_content_list:\n",
    "                # Check if the item is a dictionary with subheadings\n",
    "                if isinstance(item, dict):\n",
    "                    for sub_key, sub_values in item.items():\n",
    "                        # If sub_values is a list, it might contain more dictionaries\n",
    "                        if isinstance(sub_values, list):\n",
    "                            for value in sub_values:\n",
    "                                # If the value is a dictionary, it's another subheading\n",
    "                                if isinstance(value, dict):\n",
    "                                    for third_level_key, third_level_values in value.items():\n",
    "                                        if isinstance(third_level_values, list):\n",
    "                                            # If the third-level values are a list, iterate and format each one\n",
    "                                            for val in third_level_values:\n",
    "                                                results.append(f\"{heading} - {sub_key} - {third_level_key}: {val}\")\n",
    "                                        else:\n",
    "                                            # If it's a single value, format it directly\n",
    "                                            results.append(f\"{heading} - {sub_key} - {third_level_key}: {third_level_values}\")\n",
    "                                else:\n",
    "                                    # If it's just a string, append it with its heading and subheading\n",
    "                                    results.append(f\"{heading} - {sub_key}: {value}\")\n",
    "                        else:\n",
    "                            # If sub_values is just a string, append it with its heading\n",
    "                            results.append(f\"{heading} - {sub_key}: {sub_values}\")\n",
    "                else:\n",
    "                    # If the item is just a string, append it with its main heading\n",
    "                    results.append(f\"{heading}: {item}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# Usage\n",
    "processor = HierarchicalChunkProcessor()\n",
    "\n",
    "file_list = [\n",
    "    'chunk_1_6.json', 'chunk_6_7.json', 'chunk_7_10.json', 'chunk_10_13.json', \n",
    "    'chunk_13_14.json', 'chunk_14_15.json', 'chunk_15_17.json', 'chunk_17_19.json', \n",
    "    'chunk_19_20.json', 'chunk_20_29.json', 'chunk_29_30.json', 'chunk_30_1032_new.json', \n",
    "    'chunk_1032_1240_new.json', 'chunk_1240_1244.json', 'chunk_1244_1245.json', \n",
    "    'chunk_1245_1251.json', 'chunk_1251_1252.json', 'chunk_1252_1254.json', \n",
    "    'chunk_1254_1255.json', 'chunk_1255_1256.json', 'chunk_1256_1264.json', \n",
    "    'chunk_1264_1270.json', 'chunk_1270_1276.json', 'chunk_1276_1278.json', \n",
    "    'chunk_1278_1280.json', 'chunk_1280_1286.json', 'chunk_1286_1287.json', \n",
    "    'chunk_1287_1290.json', 'chunk_1290_1291.json', 'chunk_1291_1295.json'\n",
    "]\n",
    "\n",
    "# # Load chunks from multiple JSON files\n",
    "# file_list = [\"chunk_30_1032_new.json\", \"chunk_1032_1240_new.json\", ]\n",
    "# processor.load_from_json_files(file_list)\n",
    "\n",
    "# formatted_chunks = processor.format_chunks()\n",
    "\n",
    "# # Print the formatted chunks\n",
    "# for chunk in formatted_chunks:\n",
    "#     print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `text_chunks` is a list of your text chunks\n",
    "# and `filtered_keywords` is your list of keywords after filtering\n",
    "\n",
    "def extract_heading(text_chunk):\n",
    "    # The heading is defined as the part before the first colon\n",
    "    return text_chunk.split(':')[0]\n",
    "\n",
    "def contains_keyword(heading, keywords):\n",
    "    # Check if any keyword is in the heading\n",
    "    return any(keyword.lower() in heading.lower() for keyword in keywords)\n",
    "\n",
    "# Filter the text chunks to only keep those with a heading containing at least one keyword\n",
    "# filtered_text_chunks = [\n",
    "#     chunk for chunk in formatted_chunks\n",
    "#     if contains_keyword(extract_heading(chunk), filtered_keywords)\n",
    "# ]\n",
    "\n",
    "# for r in filtered_text_chunks:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rithvikprakki/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# For using WCS\n",
    "import weaviate\n",
    "import json\n",
    "import requests\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def vector_search(filtered_text_chunks, given_property):\n",
    "    # Define the schema with optional properties for subheading and final_subheading, and add combined_headings\n",
    "    class_obj = {\n",
    "        \"class\": \"TextChunk\",\n",
    "        \"vectorizer\": \"text2vec-openai\",\n",
    "        \"properties\": [\n",
    "            {\"name\": \"main_heading\", \"dataType\": [\"string\"], \"indexInverted\": True},\n",
    "            {\"name\": \"subheading\", \"dataType\": [\"string\"], \"indexInverted\": True, \"optional\": True},\n",
    "            {\"name\": \"final_subheading\", \"dataType\": [\"string\"], \"indexInverted\": True, \"optional\": True},\n",
    "            {\"name\": \"content\", \"dataType\": [\"text\"], \"indexInverted\": True},\n",
    "            {\"name\": \"combined_headings\", \"dataType\": [\"string\"], \"indexInverted\": True}  # New field for combined headings\n",
    "        ],\n",
    "        \"moduleConfig\": {\n",
    "            \"text2vec-openai\": {},\n",
    "            \"generative-openai\": {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    auth_config = weaviate.AuthApiKey(api_key=\"bBcwd1BhBOpq6jkC5yVdswPxE7cRrPYg1kNn\")\n",
    "\n",
    "    client = weaviate.Client(\n",
    "        url=\"https://ramvisor-vectordb-kidiscnb.weaviate.network\",\n",
    "        auth_client_secret=auth_config,\n",
    "        additional_headers={\n",
    "            \"X-OpenAI-Api-Key\": \"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\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check if the class exists and delete it if it does\n",
    "    if client.schema.exists(\"TextChunk\"):\n",
    "        client.schema.delete_class(\"TextChunk\")\n",
    "\n",
    "    # Create the new class with the combined_headings field\n",
    "    client.schema.create_class(class_obj)\n",
    "\n",
    "    def parse_text_chunk(text_chunk):\n",
    "        # Find the index of the first colon\n",
    "        colon_index = text_chunk.find(':')\n",
    "        # If there's no colon, assume the entire chunk is a heading\n",
    "        if colon_index == -1:\n",
    "            headings_combined = text_chunk.strip()\n",
    "        else:\n",
    "            # Extract everything before the first colon\n",
    "            headings_combined = text_chunk[:colon_index].strip()\n",
    "        \n",
    "        # Split by dashes to get individual headings\n",
    "        headings_parts = headings_combined.split(\" - \")\n",
    "\n",
    "        # Initialize the dictionary with combined headings\n",
    "        headings = {\n",
    "            \"main_heading\": None,\n",
    "            \"subheading\": None,\n",
    "            \"final_subheading\": None,\n",
    "            \"combined_headings\": headings_combined\n",
    "        }\n",
    "        \n",
    "        # Dynamically assign the headings based on their count\n",
    "        if headings_parts:\n",
    "            headings[\"main_heading\"] = headings_parts[0].strip()\n",
    "        if len(headings_parts) > 1:\n",
    "            headings[\"subheading\"] = headings_parts[1].strip()\n",
    "        if len(headings_parts) > 2:\n",
    "            headings[\"final_subheading\"] = \" - \".join(headings_parts[2:]).strip()  # Join any remaining headings\n",
    "\n",
    "        # Only return the headings without content\n",
    "        return {\n",
    "            \"main_heading\": headings[\"main_heading\"],\n",
    "            \"subheading\": headings[\"subheading\"],\n",
    "            \"final_subheading\": headings[\"final_subheading\"],\n",
    "            \"combined_headings\": headings[\"combined_headings\"]\n",
    "        }\n",
    "\n",
    "    # Batch import all text chunks\n",
    "    client.batch.configure(batch_size=200)\n",
    "\n",
    "    # Adapted batch import to use combine_chunks function and include combined_headings\n",
    "    with client.batch as batch:\n",
    "        for i, combined_chunk in enumerate(filtered_text_chunks):\n",
    "            # print(f\"Processing combined text chunk: {i+1}\")\n",
    "            properties = parse_text_chunk(combined_chunk)  # Adapt the parse function if necessary\n",
    "\n",
    "            # Debug: Print properties to ensure content is not too long\n",
    "            # print(\"Properties being sent to Weaviate:\", properties)\n",
    "\n",
    "            try:\n",
    "                batch.add_data_object(\n",
    "                    data_object=properties,\n",
    "                    class_name=\"TextChunk\",\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to import chunk {i+1}: {e}\")\n",
    "\n",
    "    # Perform a vector search query to find text chunks related to the input, focusing on headings\n",
    "    nearText = {\n",
    "        \"concepts\": [user_input],\n",
    "        \"properties\": [given_property]  # Focus on the combined headings for the search\n",
    "    }\n",
    "\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"TextChunk\", [\"main_heading\", \"subheading\", \"final_subheading\", \"content\"])\n",
    "        .with_near_text(nearText)\n",
    "        .with_limit(100)  # Adjust the limit as needed to return more results\n",
    "        .with_additional(\"certainty\")  # Request the similarity scores (certainty)\n",
    "        .do()\n",
    "    )\n",
    "\n",
    "    # Process the response to format it as two separate lists: one for text chunks, one for similarity scores\n",
    "    formatted_results = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    for item in response['data']['Get']['TextChunk']:\n",
    "        # Reconstruct the text chunk with headings and content\n",
    "        text_chunk = ''\n",
    "        if item.get('main_heading'):\n",
    "            text_chunk += item['main_heading']\n",
    "        if item.get('subheading'):\n",
    "            text_chunk += ' - ' + item['subheading']\n",
    "        if item.get('final_subheading'):\n",
    "            text_chunk += ' - ' + item['final_subheading']\n",
    "        \n",
    "        # Include the content if it exists\n",
    "        if item.get('content'):\n",
    "            text_chunk += ': ' + item['content']\n",
    "\n",
    "        # Append the reconstructed text chunk with content to the results list\n",
    "        formatted_results.append(text_chunk)\n",
    "        \n",
    "        # Retrieve the similarity score and append it to the scores list\n",
    "        similarity_score = item['_additional']['certainty']\n",
    "        similarity_scores.append(similarity_score)\n",
    "    return formatted_results, similarity_scores\n",
    "\n",
    "\n",
    "# # Now you have two lists: one with the text chunks and one with the similarity scores\n",
    "# # Print the formatted results\n",
    "# for res in formatted_results:\n",
    "#     print(res)\n",
    "\n",
    "# # Print the similarity scores\n",
    "# for score in similarity_scores:\n",
    "#     print(f\"Similarity score: {score:.2f}\")  # Format the score to two decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "def vector_search(user_input, given_property):\n",
    "    # Configure the client to connect to your Weaviate instance\n",
    "    auth_config = weaviate.AuthApiKey(api_key=\"bBcwd1BhBOpq6jkC5yVdswPxE7cRrPYg1kNn\")\n",
    "\n",
    "    client = weaviate.Client(\n",
    "        url=\"https://ramvisor-vectordb-kidiscnb.weaviate.network\",\n",
    "        auth_client_secret=auth_config,\n",
    "        additional_headers={\n",
    "            \"X-OpenAI-Api-Key\": \"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\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Perform a vector search query to find text chunks related to the input, focusing on headings\n",
    "    nearText = {\n",
    "        \"concepts\": [user_input],\n",
    "        \"properties\": [given_property]  # Focus on the combined headings for the search\n",
    "    }\n",
    "\n",
    "    response = (\n",
    "        client.query\n",
    "        .get(\"TextChunk\", [\"main_heading\", \"subheading\", \"final_subheading\", \"content\"])\n",
    "        .with_near_text(nearText)\n",
    "        .with_limit(100)  # Adjust the limit as needed to return more results\n",
    "        .with_additional(\"certainty\")  # Request the similarity scores (certainty)\n",
    "        .do()\n",
    "    )\n",
    "\n",
    "    # Process the response to format it as two separate lists: one for text chunks, one for similarity scores\n",
    "    formatted_results = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    for item in response['data']['Get']['TextChunk']:\n",
    "        # Reconstruct the text chunk with headings and content\n",
    "        text_chunk = ''\n",
    "        if item.get('main_heading'):\n",
    "            text_chunk += item['main_heading']\n",
    "        if item.get('subheading'):\n",
    "            text_chunk += ' - ' + item['subheading']\n",
    "        if item.get('final_subheading'):\n",
    "            text_chunk += ' - ' + item['final_subheading']\n",
    "        \n",
    "        # Include the content if it exists\n",
    "        if item.get('content'):\n",
    "            text_chunk += ': ' + item['content']\n",
    "\n",
    "        # Append the reconstructed text chunk with content to the results list\n",
    "        formatted_results.append(text_chunk)\n",
    "        \n",
    "        # Retrieve the similarity score and append it to the scores list\n",
    "        similarity_score = item['_additional']['certainty']\n",
    "        similarity_scores.append(similarity_score)\n",
    "    \n",
    "    return formatted_results, similarity_scores\n",
    "\n",
    "# Example usage\n",
    "# Replace 'your-weaviate-instance-url' and 'your-weaviate-api-key' with your actual Weaviate instance URL and API key.\n",
    "# Also, replace 'user_input' with the actual user query and 'given_property' with the property you want to search against.\n",
    "formatted_results, similarity_scores = vector_search(user_input, \"combined_headings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks_with_headings(headings, text_chunks):\n",
    "    # List to hold the matched chunks\n",
    "    matched_chunks = []\n",
    "\n",
    "    # Iterate over each heading\n",
    "    for heading in headings:\n",
    "        # Iterate over each text chunk\n",
    "        for chunk in text_chunks:\n",
    "            # Check if the chunk contains the heading before the first colon\n",
    "            if chunk.startswith(heading + ':'):\n",
    "                # If a match is found, add the full chunk to the list\n",
    "                matched_chunks.append(chunk)\n",
    "                \n",
    "    # Return the list of matched chunks\n",
    "    return matched_chunks\n",
    "\n",
    "# # Find matches\n",
    "# matched_results = find_chunks_with_headings(formatted_results, filtered_text_chunks)\n",
    "\n",
    "# # Print matched results\n",
    "# for full_chunk in matched_results:\n",
    "#     print(full_chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def filter_relevant_headings(text_chunks, question, threshold=0.6):\n",
    "    # Function to encode a list of texts into embeddings\n",
    "    def encode(texts):\n",
    "        # The model.encode function returns a 2-dimensional numpy array\n",
    "        # with shape (number_of_texts, embedding_dimension)\n",
    "        return model.encode(texts, convert_to_tensor=False)\n",
    "    \n",
    "    # Function to extract heading from a text chunk\n",
    "    # Placeholder for the actual heading extraction logic\n",
    "    def extract_heading(text_chunk):\n",
    "        # Implement the actual extraction logic here\n",
    "        return text_chunk.split('\\n')[0]  # Example: get the first line as heading\n",
    "    \n",
    "    # Encode the question once since it will be compared with multiple headings\n",
    "    question_embedding = encode([question])[0]  # Take the first (and only) embedding\n",
    "\n",
    "    # Filter and return relevant text chunks\n",
    "    relevant_chunks = []\n",
    "    for chunk in text_chunks:\n",
    "        heading = extract_heading(chunk)\n",
    "        heading_embedding = encode([heading])[0]  # Take the first (and only) embedding\n",
    "        similarity = cosine_similarity([heading_embedding], [question_embedding])[0][0]\n",
    "        if similarity > threshold:\n",
    "            relevant_chunks.append(chunk)\n",
    "\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_questions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am a sophomore studying the computer science BS program. i have finished all math requirements other than multivariable calc, and am finshing comp 301 right now, in my fall semester. next semester im taking comp 211, comp 455, and math 233. i want to graduate the end of my junior year. ive already finished all my gen ed classes. plan out my junior fall and junior spring classes to graduate.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am a sophomore studying the computer science BS program. i have finished all math requirements other than multivariable calc, and am finshing comp 301 right now, in my fall semester. next semester im taking comp 211, comp 455, and math 233. i want to graduate the end of my junior year. ive already finished all my gen ed classes. plan out my junior fall and junior spring classes to graduate.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from chunk_7_10.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_10_13.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_15_17.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_17_19.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_20_29.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_30_1032_new.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1032_1240_new.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1240_1244.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1245_1251.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1251_1252.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1252_1254.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1254_1255.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1255_1256.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1256_1264.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1264_1270.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1270_1276.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1276_1278.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1278_1280.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1280_1286.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1286_1287.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1287_1290.json. Current number of keys in data: 412\n",
      "Loaded data from chunk_1290_1291.json. Current number of keys in data: 412\n"
     ]
    }
   ],
   "source": [
    "if is_follow_up(user_input, user_questions):\n",
    "    # If it's a follow-up, skip vector search and prepare an empty string for matched_results\n",
    "    matched_results = \"\"\n",
    "    print(\"This seems to be a follow-up question. Skipping vector search.\")\n",
    "else:\n",
    "    # If it's not a follow-up, proceed with vector search and update user questions list\n",
    "    user_questions.append(user_input)\n",
    "    initial_keywords = get_keywords_from_question(user_input)\n",
    "    filtered_keywords = filter_keywords(initial_keywords)\n",
    "    file_list = [\n",
    "        'chunk_7_10.json', 'chunk_10_13.json', 'chunk_15_17.json', 'chunk_17_19.json', \n",
    "        'chunk_20_29.json', 'chunk_30_1032_new.json', 'chunk_1032_1240_new.json', 'chunk_1240_1244.json',\n",
    "        'chunk_1245_1251.json', 'chunk_1251_1252.json', 'chunk_1252_1254.json', \n",
    "        'chunk_1254_1255.json', 'chunk_1255_1256.json', 'chunk_1256_1264.json', \n",
    "        'chunk_1264_1270.json', 'chunk_1270_1276.json', 'chunk_1276_1278.json', \n",
    "        'chunk_1278_1280.json', 'chunk_1280_1286.json', 'chunk_1286_1287.json', \n",
    "        'chunk_1287_1290.json', 'chunk_1290_1291.json'\n",
    "    ]  \n",
    "    processor.load_from_json_files(file_list)\n",
    "    formatted_chunks = processor.format_chunks()\n",
    "    # for r in formatted_chunks:\n",
    "    #     print(r)\n",
    "    filtered_text_chunks = [\n",
    "        chunk for chunk in formatted_chunks\n",
    "        if contains_keyword(extract_heading(chunk), filtered_keywords)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # filtered_text_chunks2 = []\n",
    "    # for r in filtered_text_chunks:\n",
    "    #     heading = extract_heading(r)\n",
    "    #     filtered_text_chunks2.append(heading)  # Correct usage of append\n",
    "\n",
    "\n",
    "    # # for r in filtered_text_chunks2:\n",
    "    # #     print(r)\n",
    "    # headings_filtered = filter_relevant_headings(filtered_text_chunks2, user_input)\n",
    "    # for r in headings_filtered:\n",
    "    #     print(r)\n",
    "\n",
    "    formatted_results, similarity_scores = vector_search(filtered_text_chunks, \"combined_headings\")\n",
    "    matched_results = find_chunks_with_headings(formatted_results, filtered_text_chunks)\n",
    "\n",
    "\n",
    "\n",
    "    # for r in matched_results:\n",
    "    #     print(r)\n",
    "    # matched_results = '\\n'.join(matched_results)\n",
    "    # for r in formatted_results:\n",
    "    #     print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for reaching out for assistance with your academic planning. Before we proceed, could you please confirm whether you are pursuing a Bachelor of Arts (BA) or a Bachelor of Science (BS) in Computer Science? This information will help me tailor the course recommendations to your specific degree program. Once I have this clarification, I will be able to assist you in planning your remaining semesters to ensure you meet all the graduation requirements for your junior year."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "# Define your API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context:\n",
    "__________\n",
    "{matched_results}\n",
    "__________\n",
    "Q: {user_input}. \n",
    "You are an AI designed to assist with academic planning. A student has approached you for help in planning their remaining semesters in a computer science major. The student has specified the courses they have completed and the courses they are enrolled in for the upcoming semester. They intend to graduate at the end of their junior year and have already fulfilled all general education requirements. Your role is to develop a course schedule for their junior year that adheres to their major's graduation requirements without suggesting any courses they have mentioned as completed or currently enrolled in.\n",
    "\n",
    "If the student does not specify whether they are pursuing a Bachelor of Arts (BA) or a Bachelor of Science (BS) in Computer Science, you must ask for clarification before proceeding. Your recommendations should only include the remaining courses required for graduation, taking into account any prerequisites or corequisites. Do not recommend courses that have already been completed or are in progress based on the student's statements. If additional information is necessary to provide an accurate plan, ask the student to provide it.\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "system_instruct = \"\"\"\n",
    "1. Carefully note the user's academic status, including all courses they have finished and those they are currently taking or planning to take.\n",
    "2. Confirm the specific computer science degree program (BA or BS) before making any course recommendations. If this information is not available, ask the user to clarify.\n",
    "3. Cross-reference the user's current academic status with the degree requirements to identify what courses remain to be taken.\n",
    "4. Avoid recommending any courses the user has already completed or is currently enrolled in.\n",
    "5. Focus on suggesting a schedule for the junior year that fulfills the major requirements and prerequisites for graduation within the user's timeline.\n",
    "6. If the provided information is insufficient to create a full and accurate plan, ask for further details from the user.\n",
    "7. Make sure to include any additional major-specific requirements such as required science or mathematics courses that have not yet been addressed by the user's current or planned courses.\n",
    "8. Present a clear and structured academic plan that enables the student to meet all graduation requirements for their specified degree by the end of their junior year.\n",
    "\"\"\"\n",
    "\n",
    "def get_response_streamed(messages, system_message, user_prompt, temperature, frequency_penalty, presence_penalty, top_p):\n",
    "    # Append new system and user messages to the message history\n",
    "    messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    \n",
    "    # Ensure that only the last 10 messages are kept\n",
    "    messages = messages[-10:]\n",
    "\n",
    "    # Initialize the stream\n",
    "    response_stream = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        top_p=top_p,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Collect the response parts as they come in\n",
    "    full_response_content = \"\"\n",
    "    for response in response_stream:\n",
    "        if 'choices' in response and len(response['choices']) > 0:\n",
    "            choice = response['choices'][0]\n",
    "            if 'delta' in choice and 'content' in choice['delta']:\n",
    "                content = choice['delta']['content']\n",
    "                full_response_content += content\n",
    "                print(content, end='', flush=True)  # Print the content as it's generated\n",
    "            else:\n",
    "                # Print only if there is an error key\n",
    "                if 'error' in choice:\n",
    "                    print(f\"Error: {choice['error']}\")\n",
    "        else:\n",
    "            # If response is not the expected structure, you can print it out or handle it as you like\n",
    "            print(\"Received an unexpected response:\", response)\n",
    "    \n",
    "    # Return the full response content and the updated message history\n",
    "    return full_response_content, messages\n",
    "\n",
    "# Initialize an empty message history\n",
    "message_history = []\n",
    "\n",
    "# Example usage with the new parameters and message history\n",
    "response_content_2, message_history = get_response_streamed(\n",
    "    message_history,\n",
    "    system_instruct,\n",
    "    prompt,\n",
    "    temperature=0,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    top_p=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
